#!/usr/bin/env python3
"""
Unified Downstream Verification Script for Abstract-Rendering

This script provides certified verification for multiple downstream neural network
models (ResNet, GateNet, YOLO) on abstract image representations generated by the
Gaussian splatting renderer.

The script preserves the IBP+CROWN bounding logic from the original NeRF verification
scripts (nerf_forward_classification_resnet.py, nerf_forward_pose_estimation_gatenet.py,
nerf_forward_yolo.py) and adapts it to work with the Abstract-Rendering codebase.

Default behavior now matches the legacy nerf_forward_*.py scripts:
- Uses IBP+CROWN (IBP for intermediate bounds, then forward+backward with reference)
- PerturbationLinear is now available (if auto_LiRPA v0.6+ is installed)
- Box perturbation (PerturbationLpNorm) is default for abstract images

NOTE: Visualization functions have been removed from this script.
      Use visualize_verification_results.py for generating plots from bounds files.
"""

import os
import sys
import argparse
import math
from typing import Optional, Dict, Any, List, Tuple
from pathlib import Path
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# Add repo root to path
script_dir = os.path.dirname(os.path.realpath(__file__))
repo_root = os.path.abspath(os.path.join(script_dir, ".."))
if repo_root not in sys.path:
    sys.path.append(repo_root)

from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm

# Try to import PerturbationLinear (available in custom auto_LiRPA branch)
try:
    from auto_LiRPA import PerturbationLinear
    PERTURBATION_LINEAR_AVAILABLE = True
except ImportError:
    PERTURBATION_LINEAR_AVAILABLE = False
    print("Warning: PerturbationLinear not available. Linear perturbation mode disabled.")
    print("To enable, install auto_LiRPA from the van_verify_fix_six branch.")

# Import model definitions
from DownStreamModel.cifar10_resnet.resnet import resnet2b, resnet4b

# Try to import YOLO (may not be available in all environments)
try:
    import onnx
    import onnx2pytorch

    # Compatibility patch for onnx2pytorch Pad(constant=...)
    import onnx2pytorch.operations.pad as pad_module
    import onnx2pytorch.convert.operations as conv_ops

    _OrigPad = pad_module.Pad

    class CompatPad(_OrigPad):
        def __init__(self, *args, constant=None, value=None, **kwargs):
            # Swallow 'constant' and 'value' kwargs to avoid TypeError
            super().__init__(*args, **kwargs)

    # Apply patch
    pad_module.Pad = CompatPad
    conv_ops.Pad = CompatPad

    from DownStreamModel.yolo.yolo_utils import ONNXYOLOModel
    YOLO_AVAILABLE = True
except (ImportError, TypeError) as e:
    YOLO_AVAILABLE = False
    print(f"Warning: YOLO not available: {e}")

# Import codebase utilities
from scripts.utils import (
    iter_abstract_records,
    load_abstract_record,
)

def resize_abstract_bounds(
    lower: torch.Tensor,
    upper: torch.Tensor,
    target_size: int,
    mode: str = "bilinear"
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Resize abstract image bounds to target size.

    Args:
        lower: Lower bound tensor (H, W, 3)
        upper: Upper bound tensor (H, W, 3)
        target_size: Target image size (assumes square)
        mode: Interpolation mode ('bilinear' or 'nearest')

    Returns:
        Tuple of (resized_lower, resized_upper)
    """
    # Permute to (3, H, W) and add batch dimension
    lower_t = lower.permute(2, 0, 1).unsqueeze(0)
    upper_t = upper.permute(2, 0, 1).unsqueeze(0)

    # Resize using F.interpolate
    align_corners = False if mode == "bilinear" else None
    lower_resized = F.interpolate(
        lower_t, size=(target_size, target_size), mode=mode, align_corners=align_corners
    )
    upper_resized = F.interpolate(
        upper_t, size=(target_size, target_size), mode=mode, align_corners=align_corners
    )

    # Remove batch dimension and permute back to (H, W, 3)
    lower_resized = lower_resized.squeeze(0).permute(1, 2, 0)
    upper_resized = upper_resized.squeeze(0).permute(1, 2, 0)

    return lower_resized, upper_resized

# Constants
CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)
CIFAR10_STD = torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ============================================================================
# Helper Functions
# ============================================================================

def normalize_cifar_bounds(
    lower: torch.Tensor,
    upper: torch.Tensor,
    device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Apply CIFAR-10 normalization to image bounds.

    Args:
        lower: Lower bound tensor (B, 3, H, W)
        upper: Upper bound tensor (B, 3, H, W)
        device: Device to use

    Returns:
        Tuple of (normalized_lower, normalized_upper)
    """
    mean = CIFAR10_MEAN.to(device)
    std = CIFAR10_STD.to(device)

    lower_n = (lower - mean) / std
    upper_n = (upper - mean) / std

    # Ensure lower <= upper after normalization
    lb = torch.min(lower_n, upper_n)
    ub = torch.max(lower_n, upper_n)

    return lb, ub


class GateNet(nn.Module):
    """GateNet model for pose estimation."""

    def __init__(self, config):
        super(GateNet, self).__init__()

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1, bias=True)
        self.bn1 = nn.BatchNorm2d(16, momentum=config['batch_norm_decay'], eps=config['batch_norm_epsilon'])

        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=True)
        self.bn2 = nn.BatchNorm2d(32, momentum=config['batch_norm_decay'], eps=config['batch_norm_epsilon'])

        self.conv3 = nn.Conv2d(32, 16, kernel_size=3, padding=1, bias=True)
        self.bn3 = nn.BatchNorm2d(16, momentum=config['batch_norm_decay'], eps=config['batch_norm_epsilon'])

        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1, bias=True)
        self.bn4 = nn.BatchNorm2d(16, momentum=config['batch_norm_decay'], eps=config['batch_norm_epsilon'])

        self.conv5 = nn.Conv2d(16, 16, kernel_size=3, padding=1, bias=True)
        self.bn5 = nn.BatchNorm2d(16, momentum=config['batch_norm_decay'], eps=config['batch_norm_epsilon'])

        self.conv6 = nn.Conv2d(16, 16, kernel_size=3, padding=1, bias=True)
        self.bn6 = nn.BatchNorm2d(16, momentum=config['batch_norm_decay'], eps=config['batch_norm_epsilon'])

        self.flatten = nn.Flatten()

        res = self.conv(torch.zeros(config['input_shape'])[None])
        self.fc = nn.Linear(res.shape[1], int(torch.prod(torch.tensor(config['output_shape']))))

    def conv(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.avg_pool2d(x, kernel_size=2)

        x = F.relu(self.bn2(self.conv2(x)))
        x = F.avg_pool2d(x, kernel_size=2)

        x = F.relu(self.bn3(self.conv3(x)))
        x = F.avg_pool2d(x, kernel_size=2)

        x = F.relu(self.bn4(self.conv4(x)))
        x = F.avg_pool2d(x, kernel_size=2)

        x = F.relu(self.bn5(self.conv5(x)))
        x = F.avg_pool2d(x, kernel_size=2)

        x = F.relu(self.bn6(self.conv6(x)))

        x = self.flatten(x)

        return x

    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)
        return x


class TinyYOLOWrapper(nn.Module):
    """Wrap TinyYOLO ONNX model for verification.

    Takes (B, 3, H, W) input and returns flattened output (B, N).
    """
    def __init__(self, onnx_path: str):
        super().__init__()
        if not YOLO_AVAILABLE:
            raise RuntimeError("YOLO dependencies not available")
        onnx_model = onnx.load(onnx_path)
        from onnx2pytorch import ConvertModel
        self.model = ConvertModel(onnx_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, 3, H, W)
        y = self.model(x)
        # Flatten to (B, N) if needed
        if y.dim() > 2:
            y = y.view(y.size(0), -1)
        return y


def load_gatenet(checkpoint_path: str, tile_size: int, device: torch.device) -> nn.Module:
    """Load GateNet model from checkpoint.

    Args:
        checkpoint_path: Path to checkpoint file
        tile_size: Input image size
        device: Device to load model on

    Returns:
        Loaded GateNet model
    """
    config = {
        'input_shape': (3, tile_size, tile_size),
        'output_shape': (3,),  # X, Y, Z
        'l2_weight_decay': 1e-4,
        'batch_norm_decay': 0.99,
        'batch_norm_epsilon': 1e-3
    }

    model = GateNet(config=config)

    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=device)
        # Handle different checkpoint formats
        if isinstance(checkpoint, dict):
            if "model_state_dict" in checkpoint:
                state_dict = checkpoint["model_state_dict"]
            elif "state_dict" in checkpoint:
                state_dict = checkpoint["state_dict"]
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        # Strip optional "module." prefix from DataParallel
        clean_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        model.load_state_dict(clean_state_dict)
    else:
        raise FileNotFoundError(f"GateNet checkpoint not found: {checkpoint_path}")

    model.to(device)
    model.eval()
    return model


# ============================================================================
# Core Bounding Logic (IBP + CROWN)
# ============================================================================

def compute_bounds_ibp_crown(
    bounded_model: BoundedModule,
    images_lb: torch.Tensor,
    images_ub: torch.Tensor,
    print_flag: bool = False,
    use_ibp_intermediate: bool = True
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Compute certified bounds using IBP+CROWN.

    This function implements the same bounding logic as in the original
    NeRF verification scripts, using IBP for intermediate bounds and
    forward+backward (CROWN) for tighter final bounds.

    Args:
        bounded_model: BoundedModule wrapping the neural network
        images_lb: Lower bounds on input images (B, C*H*W)
        images_ub: Upper bounds on input images (B, C*H*W)
        print_flag: Whether to print progress messages
        use_ibp_intermediate: Use IBP for intermediate bounds (legacy behavior)

    Returns:
        Tuple of (lower_bounds, upper_bounds) on model outputs
    """
    # Create perturbation from image bounds
    ptb = PerturbationLpNorm(x_L=images_lb, x_U=images_ub)
    bounded_images = BoundedTensor((images_lb + images_ub) / 2, ptb)

    if use_ibp_intermediate:
        # Legacy nerf_forward behavior: IBP â†’ forward+backward with reference
        # Step 1: Compute IBP bounds for intermediate layers
        if print_flag:
            print("Computing IBP bounds...")

        lb_ibp, ub_ibp = bounded_model.compute_bounds(x=bounded_images, method="ibp")

        # Store intermediate bounds for reference
        reference_interm_bounds = {}
        for node in bounded_model.nodes():
            if (node.perturbed
                and isinstance(node.lower, torch.Tensor)
                and isinstance(node.upper, torch.Tensor)):
                reference_interm_bounds[node.name] = (node.lower, node.upper)

        if print_flag:
            print(f"IBP bounds computed. Found {len(reference_interm_bounds)} intermediate bounds.")

        # Step 2: Compute forward+backward (CROWN) bounds using IBP as reference
        if print_flag:
            print("Computing CROWN bounds with forward+backward...")

        lb, ub = bounded_model.compute_bounds(
            x=bounded_images,
            method="forward+backward",
            reference_bounds=reference_interm_bounds
        )

        if print_flag:
            print(f"CROWN bounds computed.")
            print(f"Lower bounds shape: {lb.shape}")
            print(f"Upper bounds shape: {ub.shape}")

        # Clean up
        del lb_ibp, ub_ibp, reference_interm_bounds
        torch.cuda.empty_cache()
    else:
        # Simpler backward-only method (faster but potentially looser bounds)
        if print_flag:
            print("Computing bounds with backward method...")

        lb, ub = bounded_model.compute_bounds(x=bounded_images, method="backward")

        if print_flag:
            print(f"Bounds computed.")
            print(f"Lower bounds shape: {lb.shape}")
            print(f"Upper bounds shape: {ub.shape}")

    return lb, ub


def compute_bounds_linear_perturbation(
    bounded_model: BoundedModule,
    input_lb: torch.Tensor,
    input_ub: torch.Tensor,
    images_lb: torch.Tensor,
    images_ub: torch.Tensor,
    lower_A: torch.Tensor,
    upper_A: torch.Tensor,
    lower_bias: torch.Tensor,
    upper_bias: torch.Tensor,
    print_flag: bool = False
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Compute bounds using linear perturbation from abstract records.

    This preserves the linear relaxations from the rendering stage.

    Args:
        bounded_model: BoundedModule wrapping the neural network
        input_lb: Lower bounds on abstract input parameters
        input_ub: Upper bounds on abstract input parameters
        images_lb: Lower bounds on rendered images
        images_ub: Upper bounds on rendered images
        lower_A: Lower bound linear coefficients
        upper_A: Upper bound linear coefficients
        lower_bias: Lower bound bias terms
        upper_bias: Upper bound bias terms
        print_flag: Whether to print progress

    Returns:
        Tuple of (lower_bounds, upper_bounds) on model outputs
    """
    if not PERTURBATION_LINEAR_AVAILABLE:
        raise RuntimeError(
            "PerturbationLinear not available. "
            "Install auto_LiRPA from van_verify_fix_six branch or use --use_linear_perturbation=False"
        )

    # Create linear perturbation
    ptb = PerturbationLinear(
        lower_A, upper_A, lower_bias, upper_bias,
        input_lb=input_lb, input_ub=input_ub,
        x_L=images_lb, x_U=images_ub
    )
    bounded_images = BoundedTensor((images_lb + images_ub) / 2, ptb)

    if print_flag:
        print("Computing bounds with linear perturbation...")

    # Compute bounds using backward propagation
    lb, ub = bounded_model.compute_bounds(x=bounded_images, method="backward")

    if print_flag:
        print(f"Bounds computed with linear perturbation.")
        print(f"Lower bounds shape: {lb.shape}")
        print(f"Upper bounds shape: {ub.shape}")

    return lb, ub


# ============================================================================
# Model-Specific Verification Functions
# ============================================================================

def verify_resnet_from_abstract(
    abstract_folder: str,
    model_name: str = "resnet2b",
    model_checkpoint: Optional[str] = None,
    output_path: Optional[str] = None,
    batch_size: int = 1,
    target_size: int = 32,
    print_flag: bool = False,
    save_nominal: bool = True,
    use_linear_perturbation: bool = False
) -> Dict[str, torch.Tensor]:
    """Verify ResNet model on abstract image bounds.

    Args:
        abstract_folder: Path to folder containing abstract_*.pt files
        model_name: ResNet variant ('resnet2b' or 'resnet4b')
        model_checkpoint: Path to model checkpoint (.pth file)
        output_path: Path to save results (.pt file)
        batch_size: Batch size for verification
        target_size: Target image size (default: 32)
        print_flag: Print debug information
        save_nominal: Save nominal (center) outputs
        use_linear_perturbation: Use linear perturbation from abstract records

    Returns:
        Dictionary with verification results
    """
    print(f"Verifying ResNet ({model_name}) on abstract records from {abstract_folder}")

    # Load model directly (no wrapper)
    if model_name == "resnet2b":
        model = resnet2b()
    elif model_name == "resnet4b":
        model = resnet4b()
    else:
        raise ValueError(f"Invalid model name: {model_name}")

    if model_checkpoint is not None and os.path.exists(model_checkpoint):
        print(f"Loading checkpoint from {model_checkpoint}")
        checkpoint = torch.load(model_checkpoint, map_location=device)
        # Handle different checkpoint formats
        if isinstance(checkpoint, dict) and "state_dict" in checkpoint:
            state_dict = checkpoint["state_dict"]
        else:
            state_dict = checkpoint
        # Strip optional "module." prefix from DataParallel
        clean_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        model.load_state_dict(clean_state_dict)

    model.to(device)
    model.eval()

    # Storage for results
    all_logits_lb = []
    all_logits_ub = []
    all_logits_nom = []
    all_pred_nom = []

    # Process records one at a time (simpler and more compatible with auto_LiRPA)
    for record_path, record in tqdm(iter_abstract_records(abstract_folder), desc="Verifying ResNet"):
        lower = record.get("lower")
        upper = record.get("upper")

        # Convert to (1, 3, H, W) format
        lower_img = lower.permute(2, 0, 1).unsqueeze(0).to(device=device, dtype=torch.float32)
        upper_img = upper.permute(2, 0, 1).unsqueeze(0).to(device=device, dtype=torch.float32)

        # Resize to target size
        lower_img = F.interpolate(lower_img, size=(target_size, target_size), mode="bilinear", align_corners=False)
        upper_img = F.interpolate(upper_img, size=(target_size, target_size), mode="bilinear", align_corners=False)

        # Normalize with CIFAR-10 statistics
        lower_n, upper_n = normalize_cifar_bounds(lower_img, upper_img, device)

        # Center of the interval
        x_center = 0.5 * (lower_n + upper_n)

        # Create perturbation
        ptb = PerturbationLpNorm(x_L=lower_n, x_U=upper_n)
        x_bounded = BoundedTensor(x_center, ptb)

        # Create BoundedModule for this input
        bounded_model = BoundedModule(model, x_center, device=device)

        with torch.no_grad():
            # Compute bounds on logits using IBP+CROWN
            # This matches the legacy nerf_forward_classification_resnet.py behavior:
            # 1. First compute IBP bounds for all intermediate layers (fast but loose)
            # 2. Use IBP bounds as reference for CROWN (tighter bounds via linear relaxations)
            # 3. Run forward+backward (CROWN) propagation with IBP reference
            # This gives tighter bounds than backward-only while staying tractable

            # Step 1: IBP for intermediate bounds
            logits_lb_ibp, logits_ub_ibp = bounded_model.compute_bounds(x=(x_bounded,), method="ibp")

            # Step 2: Store intermediate bounds as reference for CROWN
            reference_interm_bounds = {}
            for node in bounded_model.nodes():
                if (node.perturbed
                    and isinstance(node.lower, torch.Tensor)
                    and isinstance(node.upper, torch.Tensor)):
                    reference_interm_bounds[node.name] = (node.lower, node.upper)

            # Step 3: CROWN (forward+backward) with IBP reference bounds
            logits_lb, logits_ub = bounded_model.compute_bounds(
                x=(x_bounded,),
                method="forward+backward",
                reference_bounds=reference_interm_bounds
            )

            all_logits_lb.append(logits_lb.cpu())
            all_logits_ub.append(logits_ub.cpu())

            if save_nominal:
                logits_nom = model(x_center)
                pred_nom = logits_nom.argmax(dim=1)
                all_logits_nom.append(logits_nom.cpu())
                all_pred_nom.append(pred_nom.cpu())

        # Clean up to save memory
        del bounded_model, x_bounded, ptb, x_center, lower_n, upper_n
        torch.cuda.empty_cache()

    # Concatenate results
    results = {
        "logits_lb": torch.cat(all_logits_lb, dim=0),
        "logits_ub": torch.cat(all_logits_ub, dim=0),
    }

    if save_nominal:
        results["logits_nom"] = torch.cat(all_logits_nom, dim=0)
        results["pred_nom"] = torch.cat(all_pred_nom, dim=0)

    # Save results
    if output_path is not None:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(results, output_path)
        print(f"Results saved to {output_path}")

    return results


def verify_gatenet_from_abstract(
    abstract_folder: str,
    model_checkpoint: str,
    output_path: Optional[str] = None,
    batch_size: int = 1,
    target_size: int = 64,
    print_flag: bool = False,
    save_nominal: bool = True,
    use_linear_perturbation: bool = False
) -> Dict[str, torch.Tensor]:
    """Verify GateNet model on abstract image bounds.

    Args:
        abstract_folder: Path to folder containing abstract_*.pt files
        model_checkpoint: Path to GateNet checkpoint file
        output_path: Path to save results (.pt file)
        batch_size: Batch size for verification (unused, kept for API compatibility)
        target_size: Target image size (default: 64)
        print_flag: Print debug information (unused, kept for API compatibility)
        save_nominal: Save nominal (center) outputs
        use_linear_perturbation: Use linear perturbation (unused, kept for API compatibility)

    Returns:
        Dictionary with verification results
    """
    print(f"Verifying GateNet on abstract records from {abstract_folder}")

    # Load model directly
    model = load_gatenet(model_checkpoint, target_size, device)

    # Storage for results
    all_out_lb = []
    all_out_ub = []
    all_out_nom = []

    # Process records one at a time (simpler and more compatible with auto_LiRPA)
    for record_path, record in tqdm(iter_abstract_records(abstract_folder), desc="Verifying GateNet"):
        lower = record.get("lower")
        upper = record.get("upper")

        # Convert to (1, 3, H, W) format
        lower_img = lower.permute(2, 0, 1).unsqueeze(0).to(device=device, dtype=torch.float32)
        upper_img = upper.permute(2, 0, 1).unsqueeze(0).to(device=device, dtype=torch.float32)

        # Resize to target size
        lower_img = F.interpolate(lower_img, size=(target_size, target_size), mode="bilinear", align_corners=False)
        upper_img = F.interpolate(upper_img, size=(target_size, target_size), mode="bilinear", align_corners=False)

        # GateNet expects images in [0, 1] range (no normalization needed)
        # Ensure bounds are valid
        lower_img = torch.clamp(lower_img, 0.0, 1.0)
        upper_img = torch.clamp(upper_img, 0.0, 1.0)

        # Ensure lower <= upper
        lower_final = torch.min(lower_img, upper_img)
        upper_final = torch.max(lower_img, upper_img)

        # Center of the interval
        x_center = 0.5 * (lower_final + upper_final)

        # Create perturbation
        ptb = PerturbationLpNorm(x_L=lower_final, x_U=upper_final)
        x_bounded = BoundedTensor(x_center, ptb)

        # Create BoundedModule for this input
        bounded_model = BoundedModule(model, x_center, device=device)

        with torch.no_grad():
            # Compute bounds on pose outputs
            out_lb, out_ub = bounded_model.compute_bounds(x=(x_bounded,), method="backward")

            all_out_lb.append(out_lb.cpu())
            all_out_ub.append(out_ub.cpu())

            if save_nominal:
                out_nom = model(x_center)
                all_out_nom.append(out_nom.cpu())

        # Clean up to save memory
        del bounded_model, x_bounded, ptb, x_center, lower_final, upper_final
        torch.cuda.empty_cache()

    # Concatenate results
    results = {
        "out_lb": torch.cat(all_out_lb, dim=0),
        "out_ub": torch.cat(all_out_ub, dim=0),
    }

    if save_nominal:
        results["out_nom"] = torch.cat(all_out_nom, dim=0)

    # Save results
    if output_path is not None:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(results, output_path)
        print(f"Results saved to {output_path}")

    return results


def verify_yolo_from_abstract(
    abstract_folder: str,
    model_path: str,
    output_path: Optional[str] = None,
    batch_size: int = 1,
    target_size: int = 416,
    print_flag: bool = False,
    use_linear_perturbation: bool = False
) -> Dict[str, Any]:
    """Verify YOLO model on abstract image bounds.

    Uses targeted verification approach from nerf_forward_yolo.py:
    - Finds max confidence anchor in nominal output
    - Verifies only 9 relevant outputs (5 confidences + 4 regression values)
    - Much more efficient than verifying all ~21,125 outputs

    Args:
        abstract_folder: Path to folder containing abstract_*.pt files
        model_path: Path to YOLO ONNX model file
        output_path: Path to save results (.pt file)
        batch_size: Batch size for verification (unused, kept for API compatibility)
        target_size: Target image size (default: 416 for TinyYOLO)
        print_flag: Print debug information
        use_linear_perturbation: Use linear perturbation (unused for now)

    Returns:
        Dictionary with verification results
    """
    if not YOLO_AVAILABLE:
        raise RuntimeError(
            "YOLO verification requested but YOLO dependencies are not available. "
            "Please install the required packages (onnx, onnxruntime)."
        )

    print(f"Verifying YOLO on abstract records from {abstract_folder}")
    print(f"Target size: {target_size}x{target_size}")
    print(f"Using targeted verification (9 outputs per record)")

    # Load YOLO model using wrapper
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"YOLO model not found: {model_path}")

    yolo = TinyYOLOWrapper(model_path).to(device)
    yolo.eval()

    # Storage for results
    all_conf_lb = []
    all_conf_ub = []
    all_reg_lb = []
    all_reg_ub = []
    all_max_conf_idx = []
    all_unperturbed_conf = []

    # Process records one at a time
    for record_path, record in tqdm(iter_abstract_records(abstract_folder), desc="Verifying YOLO"):
        lower = record.get("lower")
        upper = record.get("upper")

        # Resize to (1, 3, H, W) format
        lower_img = lower.permute(2, 0, 1).unsqueeze(0).to(device=device, dtype=torch.float32)
        upper_img = upper.permute(2, 0, 1).unsqueeze(0).to(device=device, dtype=torch.float32)

        lower_img = F.interpolate(lower_img, size=(target_size, target_size), mode="bilinear", align_corners=False)
        upper_img = F.interpolate(upper_img, size=(target_size, target_size), mode="bilinear", align_corners=False)

        # Ensure bounds are valid [0, 1]
        lower_img = torch.clamp(lower_img, 0.0, 1.0)
        upper_img = torch.clamp(upper_img, 0.0, 1.0)
        lower_final = torch.min(lower_img, upper_img)
        upper_final = torch.max(lower_img, upper_img)

        # Center of the interval
        x_center = 0.5 * (lower_final + upper_final)

        # Create BoundedModule for this input
        bounded_yolo = BoundedModule(yolo, x_center, device=device)

        with torch.no_grad():
            # Get nominal output to find max confidence anchor
            nominal_output = yolo(x_center)  # (1, N) flattened

            # Determine YOLO output grid size
            # For 416x416 input, output is typically 13x13
            # For different sizes, calculate: output_size = input_size // 32
            grid_size = target_size // 32

            # YOLO output format: (B, 125, H, W) where 125 = 5*(1+20+4)
            # 5 anchors, each with: 1 conf + 20 classes + 4 bbox coords
            # After flattening: (B, 125*H*W)
            expected_outputs = 125 * grid_size * grid_size

            if nominal_output.size(1) != expected_outputs:
                print(f"Warning: Expected {expected_outputs} outputs, got {nominal_output.size(1)}")
                print(f"Grid size: {grid_size}x{grid_size}, adjusting...")
                # Recalculate grid size from actual output
                grid_size = int((nominal_output.size(1) / 125) ** 0.5)
                print(f"Adjusted grid size: {grid_size}x{grid_size}")

            # Reshape to (B, 125, H, W)
            test_output = nominal_output.reshape(1, 125, grid_size, grid_size)

            # Parse YOLO output: (B, H, W, 125) -> (B, H*W*5, 25)
            # 25 = 1 conf + 20 classes + 4 bbox
            B = 1
            KA = 5  # number of anchors
            NC = 20  # number of classes

            # Permute to (B, H, W, 125) and reshape
            prediction = test_output.permute(0, 2, 3, 1).contiguous().view(B, -1, 125)

            # Extract confidence, class, and bbox predictions
            conf_pred = prediction[..., :KA].contiguous().view(B, -1, 1)  # (B, H*W*5, 1)
            cls_pred = prediction[..., 1*KA : (1+NC)*KA].contiguous().view(B, -1, NC)
            bbox_pred = prediction[..., (1+NC)*KA:].contiguous().view(B, -1, 4)

            # Find max confidence anchor
            conf_pred_flat = conf_pred[0]  # (H*W*5, 1)
            max_conf, max_conf_idx = torch.max(conf_pred_flat, dim=0)
            max_conf_idx = max_conf_idx.item()

            # Calculate grid position from flat index
            # max_conf_idx is in range [0, H*W*5)
            # d2 = row, d3 = col, d1 = anchor_idx
            cells_per_row = grid_size * KA
            d2 = max_conf_idx // cells_per_row  # row
            d3 = (max_conf_idx - d2 * cells_per_row) // KA  # col
            d1 = max_conf_idx - d2 * cells_per_row - d3 * KA  # anchor

            # Calculate indices for all 5 confidences in the same grid cell
            # and 4 regression values for the max confidence anchor
            grid_cell_base = d2 * grid_size + d3
            conf_idx_same_grid = [
                anchor_idx * (grid_size * grid_size) + grid_cell_base
                for anchor_idx in range(5)
            ]

            reg_start = 105  # 5*1 + 5*20 = confidence + classes
            reg_idx_same_box = [
                (reg_start + d1 * 4 + coord_idx) * (grid_size * grid_size) + grid_cell_base
                for coord_idx in range(4)
            ]

            # Create specification matrix for targeted verification
            # Want to verify: 5 confidence values + 4 regression values = 9 outputs
            total_outputs = nominal_output.size(1)
            C_conf_with_reg = torch.zeros(1, 9, total_outputs, device=device, dtype=torch.float32)

            for i in range(5):
                C_conf_with_reg[0, i, conf_idx_same_grid[i]] = 1.0
            for i in range(4):
                C_conf_with_reg[0, 5 + i, reg_idx_same_box[i]] = 1.0

            # Create perturbation
            ptb = PerturbationLpNorm(x_L=lower_final, x_U=upper_final)
            x_bounded = BoundedTensor(x_center, ptb)

            # Compute bounds using specification matrix
            if print_flag:
                print(f"Computing bounds for record {record_path.name}...")
                print(f"Max confidence anchor: grid=({d2},{d3}), anchor={d1}, idx={max_conf_idx}")

            ret = bounded_yolo.compute_bounds(x=(x_bounded,), C=C_conf_with_reg, method="backward")

            # Extract results: ret is tuple (lb, ub)
            lb_all = ret[0]  # (1, 9)
            ub_all = ret[1]  # (1, 9)

            conf_lb = lb_all[0, :5]  # (5,)
            conf_ub = ub_all[0, :5]  # (5,)
            reg_lb = lb_all[0, 5:9]  # (4,)
            reg_ub = ub_all[0, 5:9]  # (4,)

            # Get unperturbed values
            unperturbed_outputs = nominal_output[0, [*conf_idx_same_grid, *reg_idx_same_box]]
            unperturbed_conf = torch.max(unperturbed_outputs[:5]).item()

            all_conf_lb.append(conf_lb.cpu())
            all_conf_ub.append(conf_ub.cpu())
            all_reg_lb.append(reg_lb.cpu())
            all_reg_ub.append(reg_ub.cpu())
            all_max_conf_idx.append(max_conf_idx)
            all_unperturbed_conf.append(unperturbed_conf)

            if print_flag:
                max_conf_bounded = torch.max(conf_ub).item()
                print(f"  Max confidence (nominal): {unperturbed_conf:.4f}")
                print(f"  Max confidence (bounded): {max_conf_bounded:.4f}")

        # Clean up
        del bounded_yolo, x_bounded, ptb, x_center, lower_final, upper_final
        torch.cuda.empty_cache()

    # Compile results
    results = {
        "conf_lb": torch.stack(all_conf_lb, dim=0),  # (N, 5)
        "conf_ub": torch.stack(all_conf_ub, dim=0),  # (N, 5)
        "reg_lb": torch.stack(all_reg_lb, dim=0),    # (N, 4)
        "reg_ub": torch.stack(all_reg_ub, dim=0),    # (N, 4)
        "max_conf_idx": all_max_conf_idx,            # List[int]
        "unperturbed_conf": all_unperturbed_conf,    # List[float]
    }

    print(f"\nYOLO Verification Complete")
    print(f"Verified {len(all_conf_lb)} abstract records")
    print(f"Confidence bounds shape: {results['conf_lb'].shape}")
    print(f"Regression bounds shape: {results['reg_lb'].shape}")

    if output_path is not None:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(results, output_path)
        print(f"Results saved to {output_path}")

    return results


# ============================================================================
# Analysis Functions
# ============================================================================

def analyze_classification_results(
    results: Dict[str, torch.Tensor],
    target_class: int,
    other_classes: Optional[List[int]] = None,
    plot_path: Optional[str] = None
) -> Dict[str, Any]:
    """Analyze classification verification results.

    Computes the verification gap (target_lb - max_other_ub) and determines
    which inputs are verifiably classified correctly.

    Args:
        results: Dictionary with 'logits_lb' and 'logits_ub'
        target_class: Target class index
        other_classes: List of other class indices (if None, uses all except target)
        plot_path: Optional path to save verification gap plot

    Returns:
        Dictionary with analysis results
    """
    logits_lb = results["logits_lb"]
    logits_ub = results["logits_ub"]

    # Get target class lower bounds
    target_lb = logits_lb[:, target_class]

    # Get max of other classes' upper bounds
    if other_classes is None:
        mask = torch.ones(logits_ub.shape[1], dtype=torch.bool)
        mask[target_class] = False
        other_ub_max = logits_ub[:, mask].max(dim=1)[0]
    else:
        other_ub_max = logits_ub[:, other_classes].max(dim=1)[0]

    # Compute verification gap
    verification_gap = target_lb - other_ub_max
    verified_correct = (verification_gap > 0).float()

    # Statistics
    verification_rate = verified_correct.mean().item()
    num_verified = int(verified_correct.sum().item())
    total = len(verified_correct)

    analysis = {
        "target_class": target_class,
        "verification_rate": verification_rate,
        "num_verified": num_verified,
        "total": total,
        "verification_gap": verification_gap,
        "verified_correct": verified_correct,
        "mean_gap": verification_gap.mean().item(),
        "min_gap": verification_gap.min().item(),
        "max_gap": verification_gap.max().item(),
    }

    print("\nClassification Verification Analysis")
    print("=" * 60)
    print(f"Target class: {target_class}")
    print(f"Verification rate: {verification_rate * 100:.2f}%")
    print(f"Verified correct: {num_verified} / {total}")
    print(f"Mean verification gap: {analysis['mean_gap']:.4f}")
    print(f"Min verification gap: {analysis['min_gap']:.4f}")
    print(f"Max verification gap: {analysis['max_gap']:.4f}")
    print("=" * 60)

    # Plot if requested
    if plot_path is not None:
        try:
            import matplotlib.pyplot as plt

            plt.figure(figsize=(10, 6))
            plt.hist(verification_gap.numpy(), bins=50, edgecolor='black')
            plt.axvline(x=0, color='r', linestyle='--', label='Verification threshold')
            plt.xlabel('Verification Gap')
            plt.ylabel('Count')
            plt.title(f'Verification Gap Distribution (Class {target_class})')
            plt.legend()
            plt.grid(True, alpha=0.3)

            os.makedirs(os.path.dirname(plot_path), exist_ok=True)
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"Verification gap plot saved to {plot_path}")
        except ImportError:
            print("Warning: matplotlib not available, skipping plot generation")

    return analysis


# ============================================================================
# Main CLI
# ============================================================================

def main():
    """Main function with command-line interface."""
    parser = argparse.ArgumentParser(
        description="Unified downstream verification from abstract images",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Verify ResNet classification (outputs to Outputs/NNCertification/airplane_grey_x/resnet_bounds.pt)
  python verify_downstream_unified.py \\
      --abstract_folder Outputs/AbstractImages/airplane_grey/x \\
      --model_type resnet \\
      --model_name resnet2b \\
      --model_checkpoint DownStreamModel/cifar10_resnet/resnet2b.pth \\
      --dataname airplane_grey

  # Verify GateNet pose estimation (outputs to Outputs/NNCertification/airplane_grey_round/gatenet_bounds.pt)
  python verify_downstream_unified.py \\
      --abstract_folder Outputs/AbstractImages/airplane_grey/round \\
      --model_type gatenet \\
      --model_checkpoint DownStreamModel/gatenet_final.pth \\
      --target_size 64

  # Verify YOLO object detection (outputs to Outputs/NNCertification/airplane_grey_x/yolo_bounds.pt)
  python verify_downstream_unified.py \\
      --abstract_folder Outputs/AbstractImages/airplane_grey/x \\
      --model_type yolo \\
      --model_checkpoint DownStreamModel/yolo/TinyYOLO.onnx

  # Custom output path (override default)
  python verify_downstream_unified.py \\
      --abstract_folder Outputs/AbstractImages/airplane_grey/roll \\
      --model_type resnet \\
      --output_path custom/path/results.pt
        """
    )

    # Required arguments
    parser.add_argument("--abstract_folder", type=str, required=True,
                       help="Path to folder containing abstract_*.pt files")
    parser.add_argument("--model_type", type=str, required=True,
                       choices=["resnet", "gatenet", "yolo"],
                       help="Type of model to verify")

    # Model configuration
    parser.add_argument("--model_name", type=str, default="resnet2b",
                       help="ResNet variant (resnet2b or resnet4b)")
    parser.add_argument("--model_checkpoint", type=str, default=None,
                       help="Path to model checkpoint file (.pth or .onnx)")
    parser.add_argument("--target_size", type=int, default=None,
                       help="Target image size (default: 32 for ResNet/YOLO, 64 for GateNet)")

    # Verification options
    parser.add_argument("--batch_size", type=int, default=1,
                       help="Batch size for verification")
    parser.add_argument("--print_flag", action="store_true",
                       help="Print debug information during verification")
    parser.add_argument("--no_nominal", action="store_true",
                       help="Skip computing nominal (center point) outputs")
    parser.add_argument("--use_linear_perturbation", action="store_true",
                       help="Use linear perturbation from abstract records (preserves renderer bounds)")
    parser.add_argument("--legacy_ibp_crown", action="store_true", default=True,
                       help="Use legacy IBP+CROWN method (IBP for intermediate bounds, then forward+backward). "
                            "Default: True (matches nerf_forward_*.py behavior)")
    parser.add_argument("--simple_backward", action="store_true",
                       help="Use simpler backward-only method (faster but potentially looser bounds)")

    # Output options
    parser.add_argument("--output_path", type=str, default=None,
                       help="Path to save verification results (.pt file). "
                            "If not specified, defaults to Outputs/NNCertification/<scene>/<model>_bounds.pt")
    parser.add_argument("--plot_path", type=str, default=None,
                       help="Path to save verification gap plot (classification only)")

    # Analysis options (classification)
    parser.add_argument("--target_class", type=int, default=None,
                       help="Target class index for verification analysis")
    parser.add_argument("--dataname", type=str, default=None,
                       help="Dataset name for automatic class selection "
                            "(airplane/airplane_grey=0, truck/truck_america=9, car/car_*=1)")

    # Visualization options

    args = parser.parse_args()

    # Set default target size based on model type
    if args.target_size is None:
        if args.model_type == "gatenet":
            args.target_size = 64
        else:
            args.target_size = 32

    # Determine target class from dataname if not specified
    if args.target_class is None and args.dataname is not None and args.model_type == "resnet":
        dataname_lower = args.dataname.lower()
        if "airplane" in dataname_lower:
            args.target_class = 0
        elif "truck" in dataname_lower:
            args.target_class = 9
        elif "car" in dataname_lower:
            args.target_class = 1

    # Set default checkpoint paths if not provided
    if args.model_checkpoint is None:
        if args.model_type == "resnet":
            args.model_checkpoint = f"DownStreamModel/cifar10_resnet/{args.model_name}.pth"
        elif args.model_type == "gatenet":
            args.model_checkpoint = "DownStreamModel/gatenet_final.pth"
        elif args.model_type == "yolo":
            args.model_checkpoint = "DownStreamModel/yolo/TinyYOLO.onnx"

    # Check if linear perturbation is requested but not available
    if args.use_linear_perturbation and not PERTURBATION_LINEAR_AVAILABLE:
        print("\nERROR: --use_linear_perturbation was requested but PerturbationLinear is not available.")
        print("Please install auto_LiRPA from the van_verify_fix_six branch or disable this option.")
        sys.exit(1)

    # Check if YOLO is requested but not available
    if args.model_type == "yolo" and not YOLO_AVAILABLE:
        print("\nERROR: YOLO verification requested but YOLO dependencies are not available.")
        print("Please install the required packages (onnx, onnxruntime) or choose a different model type.")
        sys.exit(1)

    # Set default output path to Outputs/NNCertification/ if not specified
    if args.output_path is None:
        # Extract scene name from abstract_folder path
        # e.g., "Outputs/AbstractImages/airplane_grey/roll" -> "airplane_grey_roll"
        abstract_path = Path(args.abstract_folder)
        scene_parts = []
        for part in abstract_path.parts:
            if part not in ["Outputs", "AbstractImages", ".", ".."]:
                scene_parts.append(part)
        scene_name = "_".join(scene_parts) if scene_parts else "default"

        # Create output directory and filename
        output_dir = Path("Outputs/NNCertification") / scene_name
        output_filename = f"{args.model_type}_bounds.pt"
        args.output_path = str(output_dir / output_filename)
        print(f"Output path not specified. Using default: {args.output_path}")

    # Run verification based on model type
    if args.model_type == "resnet":
        results = verify_resnet_from_abstract(
            abstract_folder=args.abstract_folder,
            model_name=args.model_name,
            model_checkpoint=args.model_checkpoint,
            output_path=args.output_path,
            batch_size=args.batch_size,
            target_size=args.target_size,
            print_flag=args.print_flag,
            save_nominal=not args.no_nominal,
            use_linear_perturbation=args.use_linear_perturbation
        )

        # Analyze classification results if target class specified
        if args.target_class is not None:
            analysis = analyze_classification_results(
                results,
                target_class=args.target_class,
                plot_path=args.plot_path
            )

            # Generate polar verification plot
        # Generate logit bounds bar chart
    elif args.model_type == "gatenet":
        results = verify_gatenet_from_abstract(
            abstract_folder=args.abstract_folder,
            model_checkpoint=args.model_checkpoint,
            output_path=args.output_path,
            batch_size=args.batch_size,
            target_size=args.target_size,
            print_flag=args.print_flag,
            save_nominal=not args.no_nominal,
            use_linear_perturbation=args.use_linear_perturbation
        )

        print("\nGateNet Verification Complete")
        print(f"Verified {len(results['out_lb'])} abstract records")
        if not args.no_nominal:
            print(f"Mean nominal pose: {results['out_nom'].mean(dim=0).tolist()}")

        # Generate pose bounds bar chart
    elif args.model_type == "yolo":
        results = verify_yolo_from_abstract(
            abstract_folder=args.abstract_folder,
            model_path=args.model_checkpoint,
            output_path=args.output_path,
            batch_size=args.batch_size,
            target_size=args.target_size,
            print_flag=args.print_flag,
            use_linear_perturbation=args.use_linear_perturbation
        )

        print("\nYOLO Verification Complete")
        print(f"Verified {len(results['conf_lb'])} abstract records")
        print(f"Mean max confidence (upper bound): {results['conf_ub'].max(dim=1)[0].mean().item():.4f}")
        print(f"Mean unperturbed confidence: {np.mean(results['unperturbed_conf']):.4f}")

    print("\nVerification complete!")


if __name__ == "__main__":
    main()
